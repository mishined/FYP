{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "# import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import glob\n",
    "from torchvision.io import read_video\n",
    "from torch import from_numpy\n",
    "import skimage\n",
    "import cv2\n",
    "from skimage import metrics as sm\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    \"\"\"MRI dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            root_dir (string): Directory with all the videos.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.paths = self.load_paths(root_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # if torch.is_tensor(idx):\n",
    "        #     idx = idx.tolist()\n",
    "\n",
    "        vid_name = self.paths[idx]\n",
    "        sample = self.load_image_batches(vid_name, 1, 24)\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # function to extract the paths for files froma path\n",
    "    def load_paths(self, data_path):\n",
    "        files = []\n",
    "        files.append(glob.glob(data_path, \n",
    "                    recursive = True))\n",
    "        return files[0]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def load_image_batches(self, path, step = 10, smooth = 5):\n",
    "        # path = \"C:/Users/Misha/OneDrive - University of Sussex/FYP/Participants/Participant_12/Processed_data/Video/Subject_12_03.mp4\"\n",
    "\n",
    "        frames, _, _ = read_video(str(path), output_format=\"TCHW\") # returns video frames, audio frames, metadata for the video and audio\n",
    "        # print(frames.shape)\n",
    "        images1 = []\n",
    "        images2 = []\n",
    "        img = np.array([np.moveaxis(frames[a].numpy()[:,:,:], 0, -1) for a in range(len(frames))])\n",
    "        # print(img.shape)\n",
    "        # print(sm.structural_similarity(img[0, 200:500, 600:900, 0], img[1, 200:500, 600:900, 0]))\n",
    "        # len(frames)-2\n",
    "        for i in range(0, len(frames)-2, step):\n",
    "            img1 = img[i, 100:600, 500:1000, :]\n",
    "            img2 = img[i+1, 100:600, 500:1000, :]\n",
    "            if (sm.structural_similarity(img1[:,:,0], img2[:,:,0]) < 0.90):\n",
    "\n",
    "                new_shape = (img1.shape[0] , img1.shape[1] , img1.shape[2])\n",
    "                blurred1 = skimage.transform.resize(image=img1, output_shape=new_shape).astype(np.float32)\n",
    "                blurred2 = skimage.transform.resize(image=img2, output_shape=new_shape).astype(np.float32)\n",
    "\n",
    "                blurred_1 = np.moveaxis(cv2.bilateralFilter(blurred1,smooth,160,160), -1, 0)\n",
    "                blurred_2 = np.moveaxis(cv2.bilateralFilter(blurred2,smooth,160,160), -1, 0)\n",
    "\n",
    "                images1.append(torch.from_numpy(blurred_1))\n",
    "                images2.append(torch.from_numpy(blurred_2))\n",
    "\n",
    "        if len(images2) < 1: #when the video is too short/ redundant video\n",
    "            img1_batch = images1\n",
    "            img2_batch = images2\n",
    "        else:\n",
    "            img1_batch = torch.stack(images1) # making predictions between 2 pairs of frames 53 and 83, and 84 and 130\n",
    "            img2_batch = torch.stack(images2)\n",
    "\n",
    "        return img1_batch, img2_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    def __init__(self, output_size) -> None:\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        print(type(sample))\n",
    "        print(sample[0].shape)\n",
    "        img1_batch = F.resize(sample[0], size=[self.output_size, self.output_size], antialias=False) # resize frames to ensure they are divisable by 8 \n",
    "        img2_batch = F.resize(sample[1], size=[self.output_size, self.output_size], antialias=False)\n",
    "\n",
    "        return (img1_batch, img2_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.models.optical_flow import Raft_Large_Weights\n",
    "from torchvision.models.optical_flow import raft_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaftTransforms(object):\n",
    "    def __call__(self, sample):\n",
    "        weights = Raft_Large_Weights.DEFAULT\n",
    "        raft_transforms = weights.transforms()\n",
    "        return raft_transforms(sample[0], sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dataset = MRIDataset(\"/Users/men22/OneDrive - University of Sussex/FYP/Participants/VIDEOS_ALL/*.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "torch.Size([2, 3, 500, 500])\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'img1_batch' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mtype\u001b[39m(video_dataset[\u001b[39m0\u001b[39;49m])\n",
      "Cell \u001b[1;32mIn[9], line 25\u001b[0m, in \u001b[0;36mMRIDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     22\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_image_batches(vid_name, \u001b[39m1\u001b[39m, \u001b[39m24\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[1;32m---> 25\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(sample)\n\u001b[0;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m sample\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\fyp\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "Cell \u001b[1;32mIn[31], line 9\u001b[0m, in \u001b[0;36mRescale.__call__\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(sample))\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(sample[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m----> 9\u001b[0m img1_batch \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mresize(img1_batch, size\u001b[39m=\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_size], antialias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m# resize frames to ensure they are divisable by 8 \u001b[39;00m\n\u001b[0;32m     10\u001b[0m img2_batch \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mresize(img2_batch, size\u001b[39m=\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_size], antialias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m (img1_batch, img2_batch)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'img1_batch' referenced before assignment"
     ]
    }
   ],
   "source": [
    "type(video_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed = transforms.Compose([Rescale(256),\n",
    "                               RaftTransforms()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dataset = MRIDataset(\"/Users/men22/OneDrive - University of Sussex/FYP/Participants/VIDEOS_ALL/*.mp4\", transform=composed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "torch.Size([2, 3, 500, 500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 256, 256])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_dataset[0][0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
