{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import random\n",
    "import string\n",
    "\n",
    "def detect_text_change(video_path, save_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fg_bg = cv2.createBackgroundSubtractorMOG2()\n",
    "    text_on = False\n",
    "    count = 0\n",
    "    current_text = \"\"\n",
    "    out = None\n",
    "    videos = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        fg_mask = fg_bg.apply(frame)\n",
    "        white_pixels = cv2.countNonZero(fg_mask[200:, -200:])\n",
    "\n",
    "        if white_pixels > 0 and not text_on:\n",
    "            text_on = True\n",
    "            text = ''\n",
    "            text += \" \" + pytesseract.image_to_string(frame[400:, 800:])\n",
    "            current_text = text.strip()\n",
    "            # first word\n",
    "            if out is None:\n",
    "                if current_text == '':\n",
    "                    add_text = ''.join(''.join(random.choice(string.ascii_lowercase) for i in range(5)))\n",
    "                    out = cv2.VideoWriter(f\"{save_path}/{current_text + add_text}.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), 30.0, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "                else: out = cv2.VideoWriter(f\"{save_path}/{current_text}.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), 30.0, (frame.shape[1], frame.shape[0]))\n",
    "                # print('out is None')\n",
    "            else:\n",
    "                if current_text != prev_text:\n",
    "                    out.release()\n",
    "                    if current_text == '':\n",
    "                        add_text = ''.join(''.join(random.choice(string.ascii_lowercase) for i in range(5)))\n",
    "                        out = cv2.VideoWriter(f\"{save_path}/{current_text + add_text}.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), 30.0, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "                    else: out = cv2.VideoWriter(f\"{save_path}/{current_text}.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), 30.0, (frame.shape[1], frame.shape[0]))\n",
    "                    # print('current_text != prev')\n",
    "                # print('else')\n",
    "            prev_text = current_text\n",
    "            # print(f\"Text {current_text} on\")\n",
    "            start_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "        \n",
    "        if white_pixels == 0 and text_on:\n",
    "            text_on = False\n",
    "            out.release()\n",
    "            # print(f\"Text {current_text} off\")\n",
    "            end_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES)) - 1\n",
    "            if current_text == '':\n",
    "                videos.append((current_text + add_text, start_frame, end_frame))\n",
    "            else: videos.append((current_text, start_frame, end_frame))\n",
    "        \n",
    "        if text_on:\n",
    "            out.write(frame)\n",
    "            \n",
    "        cv2.imshow('Frame', frame)\n",
    "        # cv2.imshow('FG Mask', fg_mask)\n",
    "        # cv2.imshow('200', fg_mask[200:, -200:])\n",
    "        cv2.imshow('400', fg_mask[400:, 900:])\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return videos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gmmll', 1, 1),\n",
       " ('gmmll', 31, 37),\n",
       " ('gmmll', 57, 103),\n",
       " ('Pen', 121, 122),\n",
       " ('Pen', 151, 173),\n",
       " ('Bigail', 228, 305),\n",
       " ('Bigail', 323, 345),\n",
       " ('qddvo', 419, 504),\n",
       " ('qddvo', 514, 536),\n",
       " ('Desg', 590, 670),\n",
       " ('Desg', 684, 706),\n",
       " ('Cath', 772, 848),\n",
       " ('Cath', 867, 889),\n",
       " ('Galw', 969, 1086),\n",
       " ('Ffair', 1145, 1225),\n",
       " ('Ffair', 1239, 1261),\n",
       " ('Faint', 1305, 1382),\n",
       " ('Faint', 1400, 1422),\n",
       " ('Theatr', 1511, 1588),\n",
       " ('Theatr', 1606, 1628),\n",
       " ('Ddoe', 1692, 1784),\n",
       " ('DY s fer)', 1787, 1809),\n",
       " ('cvrjf', 1871, 1988),\n",
       " ('Llaw', 2058, 2148),\n",
       " ('Llaw', 2153, 2175),\n",
       " ('Siop', 2220, 2298),\n",
       " ('Siop', 2315, 2337),\n",
       " ('Chi', 2407, 2483),\n",
       " ('Chi', 2491, 2491),\n",
       " ('Chi', 2502, 2524),\n",
       " ('Het', 2577, 2664),\n",
       " ('Het', 2672, 2694),\n",
       " ('Cwtsh', 2774, 2859),\n",
       " ('Cwtsh', 2869, 2891),\n",
       " ('Jam', 2951, 3038),\n",
       " ('Jam', 3046, 3068),\n",
       " ('Rhaw', 3121, 3196),\n",
       " ('Rhaw', 3216, 3238),\n",
       " ('is Xeloleya', 3311, 3401),\n",
       " ('Robot', 3406, 3428),\n",
       " ('Vaal)', 3489, 3581),\n",
       " ('Vaal)', 3584, 3606),\n",
       " ('Mawr', 3649, 3729),\n",
       " ('Mawr', 3744, 3766),\n",
       " ('Neidr', 3850, 3943),\n",
       " ('Neidr', 3945, 3967),\n",
       " ('Cyngor', 4021, 4097),\n",
       " ('Cyngor', 4116, 4138),\n",
       " ('laith', 4203, 4279),\n",
       " ('laith', 4291, 4291),\n",
       " ('laith', 4298, 4320),\n",
       " ('Wedyn', 4393, 4472),\n",
       " ('Wedyn', 4488, 4510)]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/text/Participant_10/Processed_data/Video/Subject_10_05.mp4\"\n",
    "save = \"C:/Users/men22/Documents/test1/\"\n",
    "stamps = detect_text_change(path, save)\n",
    "stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gmmll', 1, 1),\n",
       " ('gmmll', 31, 37),\n",
       " ('gmmll', 57, 103),\n",
       " ('Pen', 121, 122),\n",
       " ('Pen', 151, 173),\n",
       " ('Bigail', 228, 305),\n",
       " ('Bigail', 323, 345),\n",
       " ('qddvo', 419, 504),\n",
       " ('qddvo', 514, 536),\n",
       " ('Desg', 590, 670),\n",
       " ('Desg', 684, 706),\n",
       " ('Cath', 772, 848),\n",
       " ('Cath', 867, 889),\n",
       " ('Galw', 969, 1086),\n",
       " ('Ffair', 1145, 1225),\n",
       " ('Ffair', 1239, 1261),\n",
       " ('Faint', 1305, 1382),\n",
       " ('Faint', 1400, 1422),\n",
       " ('Theatr', 1511, 1588),\n",
       " ('Theatr', 1606, 1628),\n",
       " ('Ddoe', 1692, 1784),\n",
       " ('DY s fer)', 1787, 1809),\n",
       " ('cvrjf', 1871, 1988),\n",
       " ('Llaw', 2058, 2148),\n",
       " ('Llaw', 2153, 2175),\n",
       " ('Siop', 2220, 2298),\n",
       " ('Siop', 2315, 2337),\n",
       " ('Chi', 2407, 2483),\n",
       " ('Chi', 2491, 2491),\n",
       " ('Chi', 2502, 2524),\n",
       " ('Het', 2577, 2664),\n",
       " ('Het', 2672, 2694),\n",
       " ('Cwtsh', 2774, 2859),\n",
       " ('Cwtsh', 2869, 2891),\n",
       " ('Jam', 2951, 3038),\n",
       " ('Jam', 3046, 3068),\n",
       " ('Rhaw', 3121, 3196),\n",
       " ('Rhaw', 3216, 3238),\n",
       " ('is Xeloleya', 3311, 3401),\n",
       " ('Robot', 3406, 3428),\n",
       " ('Vaal)', 3489, 3581),\n",
       " ('Vaal)', 3584, 3606),\n",
       " ('Mawr', 3649, 3729),\n",
       " ('Mawr', 3744, 3766),\n",
       " ('Neidr', 3850, 3943),\n",
       " ('Neidr', 3945, 3967),\n",
       " ('Cyngor', 4021, 4097),\n",
       " ('Cyngor', 4116, 4138),\n",
       " ('laith', 4203, 4279),\n",
       " ('laith', 4291, 4291),\n",
       " ('laith', 4298, 4320),\n",
       " ('Wedyn', 4393, 4472),\n",
       " ('Wedyn', 4488, 4510)]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gkkjs', 1, 1),\n",
       " ('gkkjs', 31, 37),\n",
       " ('t Pen', 91, 139),\n",
       " ('Pen', 185, 207),\n",
       " ('q Bigail', 262, 343),\n",
       " ('y Bigail', 357, 379),\n",
       " ('\\\\ Tan', 453, 532),\n",
       " ('dxngj', 548, 570),\n",
       " ('t Desg', 624, 704),\n",
       " ('y Desg', 718, 740),\n",
       " ('\\\\ Cath', 806, 884),\n",
       " ('rf Cath', 901, 923),\n",
       " ('4 Galw', 1003, 1120),\n",
       " ('4 Ffair', 1179, 1295),\n",
       " ('4 Faint', 1350, 1426),\n",
       " ('rf Faint', 1445, 1467),\n",
       " ('Fheatr', 1534, 1608),\n",
       " ('Theatr', 1629, 1651),\n",
       " ('4 DYsfey=)', 1710, 1792),\n",
       " ('rf DY s fer)', 1805, 1827),\n",
       " ('akukb', 1888, 1975),\n",
       " ('akukb', 1981, 2005),\n",
       " ('t Liaw', 2092, 2169),\n",
       " ('\\\\ Llaw', 2187, 2209),\n",
       " ('4 Siop', 2254, 2330),\n",
       " ('4 Siop', 2349, 2371),\n",
       " ('\\\\ Chi', 2441, 2558),\n",
       " ('4 Het', 2601, 2694),\n",
       " ('\\\\ Het', 2696, 2718),\n",
       " ('qcwtsh', 2787, 2872),\n",
       " ('yCwtsh', 2881, 2904),\n",
       " ('4 Jam', 2973, 3048),\n",
       " ('Jam', 3068, 3090),\n",
       " ('4 Rhaw', 3155, 3230),\n",
       " ('\\\\ Rhaw', 3250, 3272),\n",
       " ('qRobot', 3345, 3425),\n",
       " ('yRobot', 3440, 3462),\n",
       " ('\\\\ Lamp', 3523, 3610),\n",
       " ('ea', 3618, 3640),\n",
       " ('q Mawr', 3683, 3761),\n",
       " ('\\\\ Mawr', 3778, 3800),\n",
       " ('qNeidr', 3869, 3944),\n",
       " ('y Neidr', 3964, 3986),\n",
       " ('¢yngor', 4055, 4131),\n",
       " ('€yngor', 4150, 4172),\n",
       " ('q laith', 4237, 4314),\n",
       " ('rf laith', 4321, 4321),\n",
       " ('rf laith', 4332, 4354),\n",
       " ('yWedyn', 4427, 4544)]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/text/Participant_6/Processed_data/Video/Subject_06_05.mp4\"\n",
    "save = \"C:/Users/men22/Documents/test1/\"\n",
    "stamps1 = detect_text_change(path, save)\n",
    "stamps1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_segments2(video_path, segments, output_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    stamps1 = segments\n",
    "\n",
    "    # define video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = None\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        current_text, start_frame, end_frame = segment\n",
    "        # if end_frame-start_frame<=3:break\n",
    "\n",
    "        # check if difference between start and end frame is less than 25\n",
    "        if out is not None and prev_text == current_text:\n",
    "            # join current segment to previous segment\n",
    "            out.release()\n",
    "            out = None\n",
    "            segments[i] = (current_text, last_start_frame, end_frame)\n",
    "            if i+1 < len(segments) and segments[i+1][0] == current_text: \n",
    "                segments[i] = (current_text, last_start_frame, segments[i+1][2])\n",
    "                segments.pop(i+1)\n",
    "            continue\n",
    "\n",
    "\n",
    "        # extract video segment\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "        out = cv2.VideoWriter(f\"{output_path}/segment_{current_text}.mp4\", fourcc, 30, (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "        for frame_idx in range(start_frame, end_frame+1):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            out.write(frame)\n",
    "\n",
    "        last_start_frame, last_end_frame, prev_text = start_frame, end_frame, current_text\n",
    "        out.release()\n",
    "\n",
    "    cap.release()\n",
    "    return stamps1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('u', 1, 1),\n",
       " ('u', 1, 37),\n",
       " ('Pen', 106, 176),\n",
       " ('Pen', 106, 222),\n",
       " ('Bigail', 277, 351),\n",
       " ('Bigail', 277, 394),\n",
       " ('y', 468, 585),\n",
       " ('Desg', 639, 718),\n",
       " ('Desg', 639, 755),\n",
       " ('Cath', 821, 938),\n",
       " ('Galw', 987, 1061),\n",
       " ('Galw', 987, 1104),\n",
       " ('Ffair', 1185, 1262),\n",
       " ('Ffair', 1185, 1301),\n",
       " ('Faint', 1365, 1442),\n",
       " ('Faint', 1365, 1482),\n",
       " ('Theatr', 1549, 1624),\n",
       " ('Theatr', 1549, 1666),\n",
       " ('DY s fer)', 1725, 1842),\n",
       " ('c', 1897, 1974),\n",
       " ('c', 1897, 2014),\n",
       " ('Llaw', 2073, 2154),\n",
       " ('Llaw', 2073, 2190),\n",
       " ('Siop', 2266, 2383),\n",
       " ('Chi', 2438, 2514),\n",
       " ('Chi', 2438, 2555),\n",
       " ('Het', 2629, 2719),\n",
       " ('Het', 2629, 2746),\n",
       " ('Cwtsh', 2802, 2919),\n",
       " ('Jam', 2976, 3052),\n",
       " ('Jam', 2976, 3093),\n",
       " ('Rhaw', 3170, 3246),\n",
       " ('Rhaw', 3170, 3287),\n",
       " ('sXeloleya', 3350, 3424),\n",
       " ('sXeloleya', 3350, 3467),\n",
       " ('Lamp', 3527, 3620),\n",
       " ('Lamp', 3527, 3644),\n",
       " ('Mawr', 3698, 3775),\n",
       " ('Mawr', 3698, 3815),\n",
       " ('Neidr', 3888, 3963),\n",
       " ('Neidr', 3888, 4005),\n",
       " ('Cyngor', 4070, 4162),\n",
       " ('Cyngor', 4070, 4187),\n",
       " ('laith', 4252, 4344),\n",
       " ('laith', 4252, 4369),\n",
       " ('Wedyn', 4428, 4504),\n",
       " ('Wedyn', 4428, 4545)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/text/Participant_6/Processed_data/Video/Subject_06_05.mp4\"\n",
    "save = \"C:/Users/men22/Documents/Videos6/\"\n",
    "segemnts = extract_segments2(path, stamps, save)\n",
    "segemnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# function to extract the paths for files froma path\n",
    "def load_paths(data_path):\n",
    "    files = []\n",
    "    files.append(glob.glob(data_path, \n",
    "                recursive = True))\n",
    "    return files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file exists? True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/text/Participant_6/Processed_data/Video/Subject_06_05.mp4\"\n",
    "print(\"file exists?\", os.path.exists(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/*text/Participant_7*\"\n",
    "files = load_paths(data_path + \"/Processed_data/Video/*.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/notext\\\\Participant_13_no_labels\\\\Processed_data\\\\Video\\\\Subject_13_01.mp4',\n",
       " 'C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/notext\\\\Participant_13_no_labels\\\\Processed_data\\\\Video\\\\Subject_13_02.mp4',\n",
       " 'C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/notext\\\\Participant_13_no_labels\\\\Processed_data\\\\Video\\\\Subject_13_03.mp4',\n",
       " 'C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/notext\\\\Participant_13_no_labels\\\\Processed_data\\\\Video\\\\Subject_13_04.mp4',\n",
       " 'C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/notext\\\\Participant_13_no_labels\\\\Processed_data\\\\Video\\\\Subject_13_05.mp4',\n",
       " 'C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/notext\\\\Participant_13_no_labels\\\\Processed_data\\\\Video\\\\Subject_13_06.mp4',\n",
       " 'C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/notext\\\\Participant_13_no_labels\\\\Processed_data\\\\Video\\\\Subject_13_07.mp4',\n",
       " 'C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/notext\\\\Participant_13_no_labels\\\\Processed_data\\\\Video\\\\Subject_13_08.mp4',\n",
       " 'C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/notext\\\\Participant_13_no_labels\\\\Processed_data\\\\Video\\\\Subject_13_09.mp4']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('l', 1, 1),\n",
       " ('l', 31, 37),\n",
       " ('l', 106, 176),\n",
       " ('Pen', 200, 222),\n",
       " ('k', 277, 351),\n",
       " ('Bigail', 372, 394),\n",
       " ('t', 468, 585),\n",
       " ('t', 639, 718),\n",
       " ('Desg', 733, 755),\n",
       " ('b', 821, 938),\n",
       " ('Galw', 987, 1061),\n",
       " ('Galw', 1082, 1104),\n",
       " ('aiclig', 1185, 1262),\n",
       " ('Ffair', 1279, 1301),\n",
       " ('Faint', 1365, 1442),\n",
       " ('Faint', 1460, 1482),\n",
       " ('Theatr', 1549, 1624),\n",
       " ('Theatr', 1644, 1666),\n",
       " ('y', 1725, 1842),\n",
       " ('y', 1897, 1974),\n",
       " ('y', 1981, 1983),\n",
       " ('y', 1992, 2014),\n",
       " ('y', 2073, 2154),\n",
       " ('Llaw', 2168, 2190),\n",
       " ('n', 2266, 2383),\n",
       " ('=\\nU', 2438, 2514),\n",
       " ('Chi', 2521, 2521),\n",
       " ('Chi', 2533, 2555),\n",
       " ('ey\\ni)\\nc', 2629, 2719),\n",
       " ('Het', 2724, 2746),\n",
       " ('p', 2802, 2919),\n",
       " ('p', 2976, 3052),\n",
       " ('Jam', 3071, 3093),\n",
       " ('j', 3170, 3246),\n",
       " ('Rhaw', 3265, 3287),\n",
       " ('sXeloleya', 3350, 3424),\n",
       " ('sXeloleya', 3445, 3467),\n",
       " ('g', 3527, 3620),\n",
       " ('Vaal)', 3622, 3644),\n",
       " ('u', 3698, 3775),\n",
       " ('Mawr', 3793, 3815),\n",
       " ('Neidr', 3888, 3963),\n",
       " ('Neidr', 3983, 4005),\n",
       " ('Cyngor', 4070, 4162),\n",
       " ('Cyngor', 4165, 4187),\n",
       " ('z', 4252, 4344),\n",
       " ('laith', 4347, 4369),\n",
       " ('Wedyn', 4428, 4504),\n",
       " ('Wedyn', 4523, 4545)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save = \"C:/Users/men22/Documents/Videos_/\"\n",
    "segemnts = detect_text_change(files[4], save)\n",
    "segemnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = \"C:/Users/men22/Documents/Videos/\"\n",
    "segemnts = extract_segments2(files[4],segemnts, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments2 = [(segemnts[a][0], segemnts[a][1]+10, segemnts[a][2]+20) for a in range(len(segemnts)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('l', 11, 11),\n",
       " ('l', 11, 186),\n",
       " ('Pen', 210, 232),\n",
       " ('k', 287, 361),\n",
       " ('Bigail', 382, 404),\n",
       " ('t', 478, 595),\n",
       " ('t', 478, 728),\n",
       " ('Desg', 743, 765),\n",
       " ('b', 831, 948),\n",
       " ('Galw', 997, 1071),\n",
       " ('Galw', 997, 1114),\n",
       " ('aiclig', 1195, 1272),\n",
       " ('Ffair', 1289, 1311),\n",
       " ('Faint', 1375, 1452),\n",
       " ('Faint', 1375, 1492),\n",
       " ('Theatr', 1559, 1634),\n",
       " ('Theatr', 1559, 1676),\n",
       " ('y', 1735, 1852),\n",
       " ('y', 1735, 2024),\n",
       " ('y', 2002, 2164),\n",
       " ('Llaw', 2178, 2200),\n",
       " ('n', 2276, 2393),\n",
       " ('=\\nU', 2448, 2524),\n",
       " ('Chi', 2531, 2531),\n",
       " ('Chi', 2531, 2565),\n",
       " ('ey\\ni)\\nc', 2639, 2729),\n",
       " ('Het', 2734, 2756),\n",
       " ('p', 2812, 2929),\n",
       " ('p', 2812, 3062),\n",
       " ('Jam', 3081, 3103),\n",
       " ('j', 3180, 3256),\n",
       " ('Rhaw', 3275, 3297),\n",
       " ('sXeloleya', 3360, 3434),\n",
       " ('sXeloleya', 3360, 3477),\n",
       " ('g', 3537, 3630),\n",
       " ('Vaal)', 3632, 3654),\n",
       " ('u', 3708, 3785),\n",
       " ('Mawr', 3803, 3825),\n",
       " ('Neidr', 3898, 3973),\n",
       " ('Neidr', 3898, 4015),\n",
       " ('Cyngor', 4080, 4172),\n",
       " ('Cyngor', 4080, 4197),\n",
       " ('z', 4262, 4354),\n",
       " ('laith', 4357, 4379),\n",
       " ('Wedyn', 4438, 4514),\n",
       " ('Wedyn', 4438, 4555)]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "def detect_voices(video_path, audio_path, output_path):\n",
    "    # load audio file\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    audio_duration = len(audio)\n",
    "    print(f\"Audio duration: {audio_duration} ms\")\n",
    "    \n",
    "    # define settings for voice detection\n",
    "    min_silence_len = 100\n",
    "    silence_thresh = -45\n",
    "    \n",
    "    # load video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # calculate the length of each frame in ms\n",
    "    frame_duration = 1000 / fps\n",
    "    \n",
    "    # define video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = None\n",
    "    \n",
    "    # iterate over the audio segments\n",
    "    for i, audio_segment in enumerate(audio[::min_silence_len]):\n",
    "        start_time = i * min_silence_len\n",
    "        end_time = start_time + min_silence_len\n",
    "        if end_time > audio_duration:\n",
    "            end_time = audio_duration\n",
    "        \n",
    "        # detect if the audio segment contains voice\n",
    "        if audio_segment.dBFS > silence_thresh:\n",
    "            start_frame = int(start_time / frame_duration)\n",
    "            end_frame = int(end_time / frame_duration)\n",
    "            \n",
    "            # extract video segment\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "            out = cv2.VideoWriter(f\"{output_path}/segment_{i}.mp4\", fourcc, 30, (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "            for frame_idx in range(start_frame, end_frame+1):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                out.write(frame)\n",
    "                    \n",
    "            out.release()\n",
    "            \n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio duration: 33201 ms\n"
     ]
    }
   ],
   "source": [
    "save = \"C:/Users/men22/Documents/test1/\"\n",
    "path = \"C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/text/Participant_10/Processed_data/Video/Subject_10_02.mp4\"\n",
    "audio = \"C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/text/Participant_10/Processed_data/Audio/Subject_10_02.wav\"\n",
    "segemnts = detect_voices(path, audio, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_voices(video_path, audio_path, output_path):\n",
    "    # load audio file\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    audio_duration = len(audio)\n",
    "    print(f\"Audio duration: {audio_duration} ms\")\n",
    "    \n",
    "    # define settings for voice detection\n",
    "    min_silence_len = 1500\n",
    "    silence_thresh = -45\n",
    "    \n",
    "    # load video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # calculate the length of each frame in ms\n",
    "    frame_duration = 1000 / fps\n",
    "    \n",
    "    # define video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = None\n",
    "    \n",
    "    # iterate over the audio segments\n",
    "    for i, audio_segment in enumerate(audio[::min_silence_len]):\n",
    "        start_time = i * min_silence_len\n",
    "        end_time = start_time + min_silence_len\n",
    "        if end_time > audio_duration:\n",
    "            end_time = audio_duration\n",
    "        \n",
    "        # detect if the audio segment contains voice\n",
    "        if audio_segment.dBFS > silence_thresh:\n",
    "            # extend the audio segment by 100ms before and after\n",
    "            extended_start_time = max(0, start_time - 100)\n",
    "            extended_end_time = min(end_time + 100, audio_duration)\n",
    "            extended_audio_segment = audio[extended_start_time:extended_end_time]\n",
    "            \n",
    "            # calculate the average loudness of the extended audio segment\n",
    "            average_loudness = extended_audio_segment.dBFS\n",
    "            \n",
    "            # if the average loudness is higher than the silence threshold, extract the video segment\n",
    "            if average_loudness > silence_thresh:\n",
    "                start_frame = int(start_time / frame_duration)\n",
    "                end_frame = int(end_time / frame_duration)\n",
    "                \n",
    "                # extract video segment\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "                out = cv2.VideoWriter(f\"{output_path}/segment_{i}.mp4\", fourcc, fps, (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "                for frame_idx in range(start_frame, end_frame+1):\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    out.write(frame)\n",
    "                        \n",
    "                out.release()\n",
    "            \n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio duration: 78747 ms\n"
     ]
    }
   ],
   "source": [
    "save = \"C:/Users/men22/Documents/from_audio/\"\n",
    "path = \"C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/text/Participant_6/Processed_data/Video/Subject_06_05.mp4\"\n",
    "audio = \"C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/text/Participant_6/Processed_data/Audio/Subject_06_05.wav\"\n",
    "segemnts = detect_voices(path, audio, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "def detect_voices(video_path, audio_path, output_path):\n",
    "    # load audio file\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    audio_duration = len(audio)\n",
    "    print(f\"Audio duration: {audio_duration} ms\")\n",
    "    \n",
    "    # define settings for voice detection\n",
    "    min_silence_len = 100\n",
    "    silence_thresh = -45\n",
    "    \n",
    "    # load video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    print(fps)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # calculate the length of each frame in ms\n",
    "    frame_duration = 1000 / fps\n",
    "    print(f'Frame duration: {frame_duration}')\n",
    "    \n",
    "    # define video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = None\n",
    "    video_frames = []\n",
    "    \n",
    "    # iterate over the audio segments\n",
    "    for i, audio_segment in enumerate(audio[::min_silence_len]):\n",
    "        start_time = i * min_silence_len\n",
    "        end_time = start_time + min_silence_len\n",
    "        if end_time > audio_duration:\n",
    "            end_time = audio_duration\n",
    "        \n",
    "        # detect if the audio segment contains voice\n",
    "        if audio_segment.dBFS > silence_thresh:\n",
    "            start_frame = int(start_time / frame_duration)\n",
    "            end_frame = int(end_time / frame_duration)\n",
    "            video_frames.append((start_frame, end_frame))\n",
    "\n",
    "    i = 0\n",
    "    while i<len(video_frames)-1:\n",
    "        start, end = video_frames[i]\n",
    "        start1, end1 = video_frames[i+1]\n",
    "        if start1 - end <= 20:\n",
    "            video_frames[i] = (start, end1)\n",
    "            video_frames.pop(i+1)\n",
    "        else:    \n",
    "            i+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i, (start_frame, end_frame) in enumerate(video_frames):\n",
    "        # start_frame = int(start_time / frame_duration)\n",
    "        # end_frame = int(end_time / frame_duration)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "        out = cv2.VideoWriter(f\"{output_path}/segment_{i}.mp4\", fourcc, fps , (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "        print(start_frame, end_frame, f'segment_{i}.mp4')\n",
    "        for frame_idx in range(start_frame, end_frame+1):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            out.write(frame)\n",
    "                \n",
    "        out.release()\n",
    "\n",
    "    print(video_frames)\n",
    "    print([e-s for s,e in video_frames]) \n",
    "            \n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio duration: 33432 ms\n",
      "59.94005994005994\n",
      "Frame duration: 16.683333333333334\n",
      "167 185 segment_0.mp4\n",
      "353 377 segment_1.mp4\n",
      "521 539 segment_2.mp4\n",
      "701 713 segment_3.mp4\n",
      "893 923 segment_4.mp4\n",
      "1066 1078 segment_5.mp4\n",
      "1240 1264 segment_6.mp4\n",
      "1426 1456 segment_7.mp4\n",
      "1606 1618 segment_8.mp4\n",
      "1780 1798 segment_9.mp4\n",
      "[(167, 185), (353, 377), (521, 539), (701, 713), (893, 923), (1066, 1078), (1240, 1264), (1426, 1456), (1606, 1618), (1780, 1798)]\n",
      "[18, 24, 18, 12, 30, 12, 24, 30, 12, 18]\n"
     ]
    }
   ],
   "source": [
    "save = \"C:/Users/men22/Documents/test1/\"\n",
    "path = \"C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/text/Participant_6/Processed_data/Video/Subject_06_02.mp4\"\n",
    "audio = \"C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/text/Participant_6/Processed_data/Audio/Subject_06_02.wav\"\n",
    "segemnts = detect_voices(path, audio, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting file C:/Users/Misha/Documents/test/segment_0.mp4\n",
      "Exporting file C:/Users/Misha/Documents/test/segment_1.mp4\n",
      "Exporting file C:/Users/Misha/Documents/test/segment_2.mp4\n",
      "Exporting file C:/Users/Misha/Documents/test/segment_3.mp4\n",
      "Exporting file C:/Users/Misha/Documents/test/segment_4.mp4\n",
      "Exporting file C:/Users/Misha/Documents/test/segment_5.mp4\n",
      "Exporting file C:/Users/Misha/Documents/test/segment_6.mp4\n",
      "Exporting file C:/Users/Misha/Documents/test/segment_7.mp4\n",
      "Exporting file C:/Users/Misha/Documents/test/segment_8.mp4\n",
      "Exporting file C:/Users/Misha/Documents/test/segment_9.mp4\n",
      "<class 'pydub.audio_segment.AudioSegment'>\n",
      "['DEFAULT_CODECS', '__add__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__new__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__weakref__', '_data', '_from_safe_wav', '_parse_position', '_repr_html_', '_spawn', '_sync', 'append', 'apply_gain', 'apply_gain_stereo', 'apply_mono_filter_to_each_channel', 'array_type', 'channels', 'compress_dynamic_range', 'converter', 'dBFS', 'duration_seconds', 'empty', 'export', 'fade', 'fade_in', 'fade_out', 'ffmpeg', 'frame_count', 'frame_rate', 'frame_width', 'from_file', 'from_file_using_temporary_files', 'from_flv', 'from_mono_audiosegments', 'from_mp3', 'from_ogg', 'from_raw', 'from_wav', 'get_array_of_samples', 'get_dc_offset', 'get_frame', 'get_sample_slice', 'high_pass_filter', 'invert_phase', 'low_pass_filter', 'max', 'max_dBFS', 'max_possible_amplitude', 'normalize', 'overlay', 'pan', 'raw_data', 'remove_dc_offset', 'reverse', 'rms', 'sample_width', 'set_channels', 'set_frame_rate', 'set_sample_width', 'silent', 'speedup', 'split_to_mono', 'strip_silence']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "save = \"C:/Users/Misha/Documents/test/\"\n",
    "# path = \"C:/Users/men22/OneDrive - University of Sussex/FYP/Participants/text/Participant_6/Processed_data/Video/Subject_06_02.mp4\"\n",
    "audio = \"C:/Users/Misha/OneDrive - University of Sussex/FYP/Participants/text/Participant_6/Processed_data/Audio/Subject_06_02.wav\"\n",
    "\n",
    "sound = AudioSegment.from_file(audio)\n",
    "\n",
    "# spliting audio files\n",
    "audio_chunks = split_on_silence(sound, min_silence_len=50, silence_thresh=-42 )\n",
    "\n",
    "#loop is used to iterate over the output list\n",
    "for i, chunk in enumerate(audio_chunks):\n",
    "   output_file = save + f\"segment_{i}.mp4\"\n",
    "   print(\"Exporting file\", output_file)\n",
    "   chunk.export(output_file, format=\"mp4\")\n",
    "\n",
    "print(type(chunk))\n",
    "print(dir(chunk))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "def extract_video_from_audio2(video_path, audio_path, output_path):\n",
    "    # Load audio file\n",
    "    sound = AudioSegment.from_file(audio_path)\n",
    "\n",
    "    # Split audio file into chunks based on silence\n",
    "    audio_chunks = split_on_silence(sound, min_silence_len=50, silence_thresh=-42)\n",
    "\n",
    "    # Load video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if cap is None: print('error')\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    print(fps)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Calculate the length of each frame in ms\n",
    "    frame_duration = 1000 / fps\n",
    "\n",
    "    # Define video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "    # Loop through audio chunks and extract video segments containing the audio\n",
    "    for i, chunk in enumerate(audio_chunks):\n",
    "        start_time = chunk.time_seconds * 1000\n",
    "        end_time = start_time + len(chunk)\n",
    "\n",
    "        start_frame = int(start_time / frame_duration)\n",
    "        end_frame = int(end_time / frame_duration)\n",
    "\n",
    "        # Set the video position to the start frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "        # Create a video writer for the current segment\n",
    "        out = cv2.VideoWriter(f\"{output_path}/segment_{i}.mp4\", fourcc, fps, (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "        # Loop through the frames in the segment and write them to the output video\n",
    "        for frame_idx in range(start_frame, end_frame+1):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            out.write(frame)\n",
    "\n",
    "        out.release()\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.94005994005994\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'start_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:/Users/Misha/OneDrive - University of Sussex/FYP/Participants/text/Participant_6/Processed_data/Video/Subject_06_02.mp4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m audio \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:/Users/Misha/OneDrive - University of Sussex/FYP/Participants/text/Participant_6/Processed_data/Audio/Subject_06_02.wav\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m extract_video_from_audio2(path, audio, save)\n",
      "Cell \u001b[1;32mIn[24], line 30\u001b[0m, in \u001b[0;36mextract_video_from_audio2\u001b[1;34m(video_path, audio_path, output_path)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m i, chunk \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(audio_chunks):\n\u001b[0;32m     27\u001b[0m     \u001b[39m# start_time = chunk.time_seconds * 1000\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     end_time \u001b[39m=\u001b[39m  \u001b[39mlen\u001b[39m(chunk)\n\u001b[1;32m---> 30\u001b[0m     start_frame \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(start_time \u001b[39m/\u001b[39m frame_duration)\n\u001b[0;32m     31\u001b[0m     end_frame \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(end_time \u001b[39m/\u001b[39m frame_duration)\n\u001b[0;32m     33\u001b[0m     \u001b[39m# Set the video position to the start frame\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'start_time' is not defined"
     ]
    }
   ],
   "source": [
    "save = \"C:/Users/Misha/Documents/test/\"\n",
    "path = \"C:/Users/Misha/OneDrive - University of Sussex/FYP/Participants/text/Participant_6/Processed_data/Video/Subject_06_02.mp4\"\n",
    "audio = \"C:/Users/Misha/OneDrive - University of Sussex/FYP/Participants/text/Participant_6/Processed_data/Audio/Subject_06_02.wav\"\n",
    "\n",
    "extract_video_from_audio2(path, audio, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk  = audio_chunks[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 0.006802721088435374\n",
      "End time: 0.03600907029478458\n"
     ]
    }
   ],
   "source": [
    "min_vol = -42 # adjust this value as needed\n",
    "start_time = None\n",
    "end_time = None\n",
    "\n",
    "# loop through each frame of the segment\n",
    "for i, frame in enumerate(chunk):\n",
    "    # if the frame's volume exceeds the minimum volume\n",
    "    if frame.dBFS >= min_vol:\n",
    "        # if this is the first frame above the threshold, mark it as the start time\n",
    "        if start_time is None:\n",
    "            start_time = i * chunk.frame_width / chunk.frame_rate\n",
    "        # if we've already marked the start time, keep updating the end time\n",
    "        end_time = i * chunk.frame_width / chunk.frame_rate\n",
    "\n",
    "# if we never found a frame above the threshold, assume the whole segment is silence\n",
    "if start_time is None:\n",
    "    start_time = 0\n",
    "    end_time = chunk.duration_seconds\n",
    "\n",
    "print(\"Start time:\", start_time)\n",
    "print(\"End time:\", end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "def extract_video_from_audio(audio_file_path, video_file_path, output_directory):\n",
    "    sound = AudioSegment.from_file(audio_file_path)\n",
    "    video_name = os.path.splitext(os.path.basename(video_file_path))[0]\n",
    "\n",
    "    # Split audio into words\n",
    "    words = split_on_silence(sound, min_silence_len=50, silence_thresh=-42)\n",
    "\n",
    "    # Extract videos for each word\n",
    "    for i, word in enumerate(words):\n",
    "        # Get start and end frame of word\n",
    "        start_frame = sound.index(word) / sound.frame_width\n",
    "        end_frame = start_frame + word.duration_seconds * sound.frame_rate\n",
    "\n",
    "        # Extract video for word\n",
    "        word_video = f\"{output_directory}/word{i}.mp4\"\n",
    "        os.system(f\"ffmpeg -i {video_file_path} -ss {start_frame} -to {end_frame} -c:v libx264 -crf 17 -c:a aac -b:a 128k {word_video}\")\n",
    "\n",
    "    print(f\"All {len(words)} videos extracted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AudioSegment' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:/Users/Misha/OneDrive - University of Sussex/FYP/Participants/text/Participant_6/Processed_data/Video/Subject_06_02.mp4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m audio \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:/Users/Misha/OneDrive - University of Sussex/FYP/Participants/text/Participant_6/Processed_data/Audio/Subject_06_02.wav\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m extract_video_from_audio(audio, path, save)\n",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m, in \u001b[0;36mextract_video_from_audio\u001b[1;34m(audio_file_path, video_file_path, output_directory)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# Extract videos for each word\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m i, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(words):\n\u001b[0;32m     14\u001b[0m     \u001b[39m# Get start and end frame of word\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     start_frame \u001b[39m=\u001b[39m sound\u001b[39m.\u001b[39;49mindex(word) \u001b[39m/\u001b[39m sound\u001b[39m.\u001b[39mframe_width\n\u001b[0;32m     16\u001b[0m     end_frame \u001b[39m=\u001b[39m start_frame \u001b[39m+\u001b[39m word\u001b[39m.\u001b[39mduration_seconds \u001b[39m*\u001b[39m sound\u001b[39m.\u001b[39mframe_rate\n\u001b[0;32m     18\u001b[0m     \u001b[39m# Extract video for word\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AudioSegment' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "save = \"C:/Users/Misha/Documents/test/\"\n",
    "path = \"C:/Users/Misha/OneDrive - University of Sussex/FYP/Participants/text/Participant_6/Processed_data/Video/Subject_06_02.mp4\"\n",
    "audio = \"C:/Users/Misha/OneDrive - University of Sussex/FYP/Participants/text/Participant_6/Processed_data/Audio/Subject_06_02.wav\"\n",
    "\n",
    "extract_video_from_audio(audio, path, save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
