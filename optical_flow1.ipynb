{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HEllo\n"
          ]
        }
      ],
      "source": [
        "print(\"HEllo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SVxj9xksayJp"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'cv2'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import torch\n",
        "\n",
        "\n",
        "\n",
        "# from raft.core.raft import RAFT\n",
        "# from raft.core.utils import flow_viz\n",
        "# from raft.core.utils.utils import InputPadder\n",
        "# from raft.config import RAFTConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_oKk9Ul9L2hR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('/kaggle/input/raft-pytorch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ALvG0k9Lzes"
      },
      "outputs": [],
      "source": [
        "# dont run\n",
        "import raft\n",
        "\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from scipy import signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 0-1: truncated \\UXXXXXXXX escape (4272471747.py, line 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(\"file exists?\", os.path.exists(\"\\Users\\men22\\OneDrive - University of Sussex\\FYP\\Participants\\Participant_12\\Processed_data\\Video\"))\u001b[0m\n\u001b[1;37m                                                                                                                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 0-1: truncated \\UXXXXXXXX escape\n"
          ]
        }
      ],
      "source": [
        "# Check if the file path exists for the initial video\n",
        "import os\n",
        "print(\"file exists?\", os.path.exists(\"\\Users\\men22\\OneDrive - University of Sussex\\FYP\\Participants\\Participant_12\\Processed_data\\Video\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU5tS1QJwW_i",
        "outputId": "840c157f-fbbe-467e-9288-abddaa64b71d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'cv2' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Check if the video can be read using opencv\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# video = cv2.VideoCapture('/Users/misheton/Library/CloudStorage/OneDrive-UniversityofSussex/FYP/Participants/Participant_12/Processed_data/Video/Subject_12_01.mp4')\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m video \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mVideoCapture(\u001b[39m\"\u001b[39m\u001b[39m/Users/men22/OneDrive - University of Sussex/FYP/Participants/Participant_12/Processed_data/Video/Subject_12_01.mp4\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39mtype\u001b[39m(video)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
          ]
        }
      ],
      "source": [
        "# Check if the video can be read using opencv\n",
        "# video = cv2.VideoCapture('/Users/misheton/Library/CloudStorage/OneDrive-UniversityofSussex/FYP/Participants/Participant_12/Processed_data/Video/Subject_12_01.mp4')\n",
        "video = cv2.VideoCapture(\"/Users/men22/OneDrive - University of Sussex/FYP/Participants/Participant_12/Processed_data/Video/Subject_12_01.mp4\")\n",
        "type(video)\n",
        "# /Users/misheton/Library/CloudStorage/OneDrive-UniversityofSussex/FYP/Participants/Participant_12/Processed_data/Video/Subject_12_01.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phMbas-o1klR",
        "outputId": "8b65a7d8-8cc0-4072-fb5b-168b34cc196a"
      },
      "outputs": [],
      "source": [
        "video.open(0)\n",
        "# Check if camera opened successfully\n",
        "if (video.isOpened()== False):\n",
        "\tprint(\"Error opening video file\")\n",
        "\n",
        "# Read until video is completed\n",
        "while(video.isOpened()):\n",
        "\t\n",
        "# Capture frame-by-frame\n",
        "\tret, frame = video.read()\n",
        "\t# Display the resulting frame\n",
        "\tcv2.imshow('Frame', frame)\n",
        "\t\t\n",
        "\t# Press Q on keyboard to exit\n",
        "\tif cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "\t\t\tbreak\n",
        "\n",
        "# Break the loop\n",
        "\telse:\n",
        "\t\tbreak\n",
        "\n",
        "# When everything done, release\n",
        "# the video capture object\n",
        "video.release()\n",
        "\n",
        "# Closes all the frames\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if (video.isOpened()== False):\n",
        "\tprint(\"Error opening video file\")\n",
        "\n",
        "# Read until video is completed\n",
        "while(video.isOpened()):\n",
        "\t\n",
        "# Capture frame-by-frame\n",
        "\tret, frame = video.read()\n",
        "\t# Display the resulting frame\n",
        "\tcv2.imshow('Frame', frame)\n",
        "\t\t\n",
        "\t# Press Q on keyboard to exit\n",
        "\tif cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "\t\t\tbreak\n",
        "\n",
        "# Break the loop\n",
        "\telse:\n",
        "\t\tbreak\n",
        "\n",
        "# When everything done, release\n",
        "# the video capture object\n",
        "video.release()\n",
        "\n",
        "# Closes all the frames\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2edDUy_-5YEG"
      },
      "source": [
        "Get all the frames in frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51SpEkxY4ibr"
      },
      "outputs": [],
      "source": [
        "frames = []\n",
        "#frames = np.array(frames)\n",
        "path = '/Users/misheton/OneDrive-UniversityofSussex/FYP/Participants/Participant_12/Processed_data/Video/Subject_12_01.mp4'\n",
        "cap = cv2.VideoCapture(path)\n",
        "ret = True\n",
        "while ret:\n",
        "    # ret, frame = cap.read()\n",
        "    # (height, width) = frame.shape[:2]\n",
        "    # sky = frame[150:500, 600:900]\n",
        "    # cv2.imshow('Video', sky)\n",
        "  ret, img = cap.read() # read one frame from the 'capture' object; img is (H, W, C)\n",
        "  if ret:\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    frames.append(gray)\n",
        "video = np.stack(frames, axis=0) # dimensions (T, H, W, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya3dCc74_KD-",
        "outputId": "45b0db8f-0e43-4e39-ee2a-25863583de0f"
      },
      "outputs": [],
      "source": [
        "# Cut the video to only show the mouth properties - cut around the mouth\n",
        "video = video[:, 150:500, 600:900]\n",
        "video.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7ZEgHwaOvit"
      },
      "source": [
        "1 in 6 frames - done\n",
        "simple \n",
        "cut video around the mouth - done\n",
        "properties - how well can predict\n",
        "and how smooth it is\n",
        "\n",
        "\n",
        "calculate the gradient of the optical flow in x and y\n",
        "first derivative - how quickly is the flow field changing in direction x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l5Snha7c40fs",
        "outputId": "51f62ae7-d5f0-4e4b-e459-423b7b0f228e"
      },
      "outputs": [],
      "source": [
        "# Display the resulting frames - 1 in 6 frames as there isn't movement for every frame\n",
        "for i in range(0,200,1):\n",
        "  print(i)\n",
        "  plt.imshow(video[i], cmap='gray')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1eeabiO8wjd"
      },
      "outputs": [],
      "source": [
        "# RAFT but not working\n",
        "def inference(args):\n",
        "    # get the RAFT model\n",
        "    model = RAFT(args)\n",
        "    # load pretrained weights\n",
        "    pretrained_weights = torch.load(args.model)\n",
        "\n",
        "config = RAFTConfig(\n",
        "    dropout=0,\n",
        "    alternate_corr=False,\n",
        "    small=False,\n",
        "    mixed_precision=False\n",
        ")\n",
        "\n",
        "model = RAFT(config)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PCIcV37Dm-g"
      },
      "outputs": [],
      "source": [
        "# using signal but not working\n",
        "\n",
        "import signal\n",
        "\n",
        "def optical_flow(I1g, I2g, window_size, tau=1e-2):\n",
        " \n",
        "    kernel_x = np.array([[-1., 1.], [-1., 1.]])\n",
        "    kernel_y = np.array([[-1., -1.], [1., 1.]])\n",
        "    kernel_t = np.array([[1., 1.], [1., 1.]])#*.25\n",
        "    w = window_size/2 # window_size is odd, all the pixels with offset in between [-w, w] are inside the window\n",
        "    I1g = I1g / 255. # normalize pixels\n",
        "    I2g = I2g / 255. # normalize pixels\n",
        "    # Implement Lucas Kanade\n",
        "    # for each point, calculate I_x, I_y, I_t\n",
        "    mode = 'same'\n",
        "    fx = signal.convolve2d(I1g, kernel_x, boundary='symm', mode=mode)\n",
        "    fy = signal.convolve2d(I1g, kernel_y, boundary='symm', mode=mode)\n",
        "    ft = signal.convolve2d(I2g, kernel_t, boundary='symm', mode=mode) + signal.convolve2d(I1g, -kernel_t, boundary='symm', mode=mode)\n",
        "    u = np.zeros(I1g.shape)\n",
        "    v = np.zeros(I1g.shape)\n",
        "    # within window window_size * window_size\n",
        "    for i in range(w, I1g.shape[0]-w):\n",
        "        for j in range(w, I1g.shape[1]-w):\n",
        "            Ix = fx[i-w:i+w+1, j-w:j+w+1].flatten()\n",
        "            Iy = fy[i-w:i+w+1, j-w:j+w+1].flatten()\n",
        "            It = ft[i-w:i+w+1, j-w:j+w+1].flatten()\n",
        "            #b = ... # get b here\n",
        "            #A = ... # get A here\n",
        "            # if threshold τ is larger than the smallest eigenvalue of A'A:\n",
        "            nu = ... # get velocity here\n",
        "            u[i,j]=nu[0]\n",
        "            v[i,j]=nu[1]\n",
        " \n",
        "    return (u,v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmQ64K-BFmNq"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms.functional as F\n",
        "\n",
        "plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
        "# sphinx_gallery_thumbnail_number = 2\n",
        "\n",
        "\n",
        "def plot(imgs, **imshow_kwargs):\n",
        "    if not isinstance(imgs[0], list):\n",
        "        # Make a 2d grid even if there's just 1 row\n",
        "        imgs = [imgs]\n",
        "\n",
        "    num_rows = len(imgs)\n",
        "    num_cols = len(imgs[0])\n",
        "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
        "    for row_idx, row in enumerate(imgs):\n",
        "        for col_idx, img in enumerate(row):\n",
        "            ax = axs[row_idx, col_idx]\n",
        "            img = F.to_pil_image(img.to(\"cpu\"))\n",
        "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
        "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GFDh_weJH7h"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "video1 = tf.reshape(video, [1888, 3, 720, 1280])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "8X_sgBFCGBK2",
        "outputId": "83911aa6-19a9-4b1f-c8fd-49e7858175c8"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "from pathlib import Path\n",
        "from urllib.request import urlretrieve\n",
        "from torchvision.io import read_video\n",
        "\n",
        "video_path = \"drive/MyDrive/FYP/Subject_10_01.mp4\"\n",
        "# frames, _, _ = read_video(str('drive/MyDrive/FYP/Subject_10_01.mp4'), output_format=\"TCHW\")\n",
        "\n",
        "img1_batch = torch.stack([torch.from_numpy(video[100]), torch.from_numpy(video[150])])\n",
        "img1_batch.shape\n",
        "img2_batch = torch.stack([torch.from_numpy(video1[101]), torch.from_numpy(video1[151])])\n",
        "\n",
        "plot(img1_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wznU8v0t1LkX"
      },
      "source": [
        "CV Lab Tracking - Lucas-Kanade\n",
        "- works best in slow motion\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BwcQgXwk1O0A",
        "outputId": "d1e92a8c-76ff-49f7-b58c-81d33aea7c05"
      },
      "outputs": [],
      "source": [
        "feature_params = dict(maxCorners = 30, qualityLevel = 0.2, minDistance = 6, blockSize = 7)\n",
        "p0 = cv2.goodFeaturesToTrack(video[0,...],mask = None, **feature_params)\n",
        "color = np.random.randint(0,255,(100,3))\n",
        "frame_start = 63\n",
        "frame0 = np.copy(video[frame_start,...])\n",
        "frame_end = 64\n",
        "for i in p0:\n",
        "    x,y = i.ravel()\n",
        "    cv2.circle(frame0,(int(x),int(y)),10,(255,0,255),-1)\n",
        "plt.imshow(frame0, cmap='gray'),plt.show()\n",
        "\n",
        "lk_params = dict( winSize  = (30,30),\n",
        "                  maxLevel = 5,\n",
        "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "mask = np.zeros_like(frame0)\n",
        "frame = np.copy(video[frame_end,...])\n",
        "p1, st, err = cv2.calcOpticalFlowPyrLK(video[0,...], video[frame_end,...], p0, None, **lk_params)\n",
        "good_new = p1[st==1]\n",
        "good_old = p0[st==1]\n",
        "# draw the tracks\n",
        "for i,(new,old) in enumerate(zip(good_new, good_old)):\n",
        "    a,b = new.ravel()\n",
        "    c,d = old.ravel()\n",
        "    mask = cv2.line(mask, (int(a),int(b)),(int(c),int(d)), color[i].tolist(), 10)\n",
        "    frame = cv2.circle(frame,(int(a),int(b)),5,color[i].tolist(),-1)\n",
        "img = cv2.add(frame,mask)\n",
        "\n",
        "plt.imshow(img, cmap='gray'), plt.show()\n",
        "\n",
        "# plt.imshow(video[frame_start,...], cmap='gray'), plt.show()\n",
        "# plt.imshow(video[frame_end,...], cmap='gray'), plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "Z4Z4cTamBke5",
        "outputId": "46d86721-e052-489f-fc95-5020ec6e646e"
      },
      "outputs": [],
      "source": [
        "plt.plot(err)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TQ-hRqfg936E",
        "outputId": "51c0c520-a191-40a2-b324-634db90f113b"
      },
      "outputs": [],
      "source": [
        "# p0 = cv2.goodFeaturesToTrack(video[39,...],1000, 0.15, 30)\n",
        "# color = np.random.randint(0,255,(100,3))\n",
        "frame_start = 63\n",
        "frame0 = np.copy(video[frame_start,...])\n",
        "frame_end = 80\n",
        "for i in p0:\n",
        "    x,y = i.ravel()\n",
        "    cv2.circle(frame0,(int(x),int(y)),10,(255,0,255),-1)\n",
        "plt.imshow(frame0, cmap='gray'),plt.show()\n",
        "\n",
        "lk_params = dict( winSize  = (30,30),\n",
        "                  maxLevel = 5,\n",
        "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "mask = np.zeros_like(frame0)\n",
        "frame = np.copy(video[frame_end,...])\n",
        "p1, st, err = cv2.calcOpticalFlowPyrLK(video[39,...], video[frame_end,...], p0, None, **lk_params)\n",
        "good_new = p1[st==1]\n",
        "good_old = p0[st==1]\n",
        "# draw the tracks\n",
        "for i,(new,old) in enumerate(zip(good_new, good_old)):\n",
        "    a,b = new.ravel()\n",
        "    c,d = old.ravel()\n",
        "    mask = cv2.line(mask, (int(a),int(b)),(int(c),int(d)), color[i].tolist(), 10)\n",
        "    frame = cv2.circle(frame,(int(a),int(b)),5,color[i].tolist(),-1)\n",
        "img = cv2.add(frame,mask)\n",
        "\n",
        "plt.imshow(img, cmap='gray'), plt.show()\n",
        "\n",
        "plt.imshow(video[frame_start,...], cmap='gray'), plt.show()\n",
        "plt.imshow(video[frame_end,...], cmap='gray'), plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxyrtJYPAH6Q"
      },
      "source": [
        "Dense Optical Flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk9CyCwtGMa3"
      },
      "outputs": [],
      "source": [
        "frames = []\n",
        "#frames = np.array(frames)\n",
        "# path = \"drive/MyDrive/FYP/Subject_10_01.mp4\"\n",
        "cap = cv2.VideoCapture(path)\n",
        "ret = True\n",
        "while ret:\n",
        "    # ret, frame = cap.read()\n",
        "    # (height, width) = frame.shape[:2]\n",
        "    # sky = frame[150:500, 600:900]\n",
        "    # cv2.imshow('Video', sky)\n",
        "  ret, img = cap.read() # read one frame from the 'capture' object; img is (H, W, C)\n",
        "  if ret:\n",
        "    # gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    frames.append(img)\n",
        "video = np.stack(frames, axis=0) # dimensions (T, H, W, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDU591xDGSfJ",
        "outputId": "365e1f6b-c168-4c5d-858e-3a8331cffec1"
      },
      "outputs": [],
      "source": [
        "video = video[:, 150:500, 600:900]\n",
        "video.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T3lySwQ9CBQk",
        "outputId": "26a38b11-f16c-4751-e44b-e0f053735b15"
      },
      "outputs": [],
      "source": [
        "frame_start = 63\n",
        "frame0 = np.copy(video[frame_start,...,:])\n",
        "frame_end = 64\n",
        "\n",
        "mask = np.zeros_like(frame0)\n",
        "# print(mask.shape)\n",
        "# Sets image saturation to maximum\n",
        "mask[..., 1] = 255\n",
        "\n",
        "frame = np.copy(video[frame_end,...])\n",
        "\n",
        "prev_gray = cv2.cvtColor(frame0, cv2.COLOR_BGR2GRAY)\n",
        "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "\n",
        "# Computes the magnitude and angle of the 2D vectors\n",
        "magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "# Sets image hue according to the optical flow direction\n",
        "# print(angle.shape)\n",
        "# print(mask[..., 0])\n",
        "mask[..., 0] = angle * 180 / np.pi / 2\n",
        "# Sets image value according to the optical flow magnitude (normalized)\n",
        "mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
        "# print(mask)\n",
        "\n",
        "# Converts HSV to RGB (BGR) color representation\n",
        "rgb = cv2.cvtColor(mask, cv2.COLOR_HSV2BGR)\n",
        "# Opens a new window and displays the output frame\n",
        "# cv2.imshow(\"dense optical flow\", rgb)\n",
        "\n",
        "\n",
        "img = cv2.add(frame,mask)\n",
        "\n",
        "plt.imshow(rgb), plt.show()\n",
        "plt.imshow(img, cmap= 'gray'), plt.show()\n",
        "\n",
        "\n",
        "plt.imshow(video[frame_start,...,:], cmap='gray'), plt.show()\n",
        "plt.imshow(video[frame_end,..., :], cmap='gray'), plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.16 ('fyp')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "b34a3b97fc0f5a6809853d2003a2ca7f3821f69c87418282b1516e639e3c1a94"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
